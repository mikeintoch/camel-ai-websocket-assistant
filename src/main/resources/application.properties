# routes to load
camel.main.routes-include-pattern = routes/*.yaml

#Configure Ollama local model
#quarkus.langchain4j.ollama.chat-model.model-id=granite-code:3b

#quarkus.langchain4j.ollama.chat-model.temperature=0.0
#quarkus.langchain4j.ollama.log-requests=true
#quarkus.langchain4j.log-responses=true
#quarkus.langchain4j.ollama.timeout=180s

#Configure for remote model.
quarkus.langchain4j.openai.base-url=[YOUR_REMOTE_LLM_URL]
quarkus.langchain4j.openai.api-key=[YOUR_API_KEY]
quarkus.langchain4j.openai.chat-model.model-name=[YOUR_MODEL_NAME]
quarkus.langchain4j.openai.timeout=180s
quarkus.langchain4j.openai.log-requests=true
quarkus.langchain4j.openai.log-responses=true
quarkus.langchain4j.openai.chat-model.temperature=0.0
quarkus.langchain4j.openai.chat-model.max-tokens=4096


#Vert.x configuration
camel.component.vertx-websocket.default-host=0.0.0.0
camel.component.vertx-websocket.default-port=8080