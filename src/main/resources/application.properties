# routes to load
camel.main.routes-include-pattern = routes/*.yaml

#Configure Ollama local model
#quarkus.langchain4j.ollama.chat-model.model-id=granite-code:3b

#quarkus.langchain4j.ollama.chat-model.temperature=0.0
#quarkus.langchain4j.ollama.log-requests=true
#quarkus.langchain4j.log-responses=true
#quarkus.langchain4j.ollama.timeout=180s

#Configure for remote model.
quarkus.langchain4j.openai.base-url=https://granite-8b-code-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1
quarkus.langchain4j.openai.api-key=5ded43765726456cc5d0c7f32450c371
quarkus.langchain4j.openai.chat-model.model-name=granite-8b-code-instruct-128k
quarkus.langchain4j.openai.timeout=180s
quarkus.langchain4j.openai.log-requests=true
quarkus.langchain4j.openai.log-responses=true
quarkus.langchain4j.openai.chat-model.temperature=0.0
quarkus.langchain4j.openai.chat-model.max-tokens=4096


#Vert.x configuration
camel.component.vertx-websocket.default-host=0.0.0.0
camel.component.vertx-websocket.default-port=8080